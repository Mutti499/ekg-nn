{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset used: https://www.kaggle.com/datasets/arjunascagnetto/ptbxl-atrial-fibrillation-detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ecg_plot\n",
    "import ecg_plot as ecg_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('/content/drive/MyDrive/datasets/ekg_dataset/ecgeq-500hzsrfava.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/datasets/ekg_dataset/coorteeqsrafva.csv', sep=';')\n",
    "\n",
    "# Filter rows where the diagnosis is \"AFIB\"\n",
    "afib_rows = df[df['diagnosi'] == 'AFIB']\n",
    "\n",
    "# Filter rows where the diagnosis is not \"AFIB\"\n",
    "non_afib_rows = df[df['diagnosi'] != 'AFIB']\n",
    "\n",
    "# Extract the indexes of \"AFIB\" rows\n",
    "afib_indexes = afib_rows.index\n",
    "\n",
    "# Randomly select 1500 indexes from the \"NON-AFIB\" rows\n",
    "random_non_afib_indexes = random.sample(non_afib_rows.index.tolist(), 1500)\n",
    "\n",
    "# Combine the indexes of \"AFIB\" and randomly selected \"NON-AFIB\" rows\n",
    "all_indexes = afib_indexes.union(random_non_afib_indexes)\n",
    "\n",
    "# Create a new DataFrame with the concatenated indexes\n",
    "new_df = df.loc[all_indexes]\n",
    "\n",
    "# Specify the path for the new CSV file\n",
    "new_csv_file_path = \"/content/drive/MyDrive/datasets/ekg_dataset/new_data.csv\"\n",
    "\n",
    "# Write the new DataFrame to a CSV file\n",
    "new_df.to_csv(new_csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for i in all_indexes:\n",
    "  ekgS = data[i]\n",
    "\n",
    "  ekgST = ekgS.T\n",
    "  name = f\"ekg_{i}\"\n",
    "\n",
    "  if i in afib_indexes:\n",
    "    ecg_plot.plot(ekgST, sample_rate = 500, show_grid = False, show_separate_line=False,show_lead_name=False,columns=1)\n",
    "    ecg_plot.save_as_png(name,'/content/drive/MyDrive/datasets/ekg_images_square/afib/')\n",
    "\n",
    "  else:\n",
    "    ecg_plot.plot(ekgST, sample_rate = 500, show_grid = False, show_separate_line=False,show_lead_name=False,columns=1)\n",
    "    ecg_plot.save_as_png(name,'/content/drive/MyDrive/datasets/ekg_images_square/non_afib/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "src_dataset_dir = '/content/drive/MyDrive/datasets/ekg_images_square/'\n",
    "train_dir = '/content/drive/MyDrive/datasets/EKG/datasetOriginal/train'\n",
    "test_dir = '/content/drive/MyDrive/datasets/EKG/datasetOriginal/test'\n",
    "validation_dir = '/content/drive/MyDrive/datasets/EKG/datasetOriginal/validation'\n",
    "\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(validation_dir, exist_ok=True)\n",
    "\n",
    "for folders in os.listdir(src_dataset_dir):\n",
    "    disease_path = os.path.join(src_dataset_dir, folders)\n",
    "\n",
    "    train_folders = os.path.join(train_dir, folders)\n",
    "    test_folders = os.path.join(test_dir, folders)\n",
    "    validation_folders = os.path.join(validation_dir, folders)\n",
    "    os.makedirs(train_folders, exist_ok=True)\n",
    "    os.makedirs(test_folders, exist_ok=True)\n",
    "    os.makedirs(validation_folders, exist_ok=True)\n",
    "\n",
    "    print(train_folders)\n",
    "\n",
    "    # List all images in the breed folder\n",
    "    images = os.listdir(disease_path)\n",
    "    # Randomly shuffle the images\n",
    "    random.shuffle(images)\n",
    "    # Calculate the number of images to move to the testing dataset (20%)\n",
    "    num_test_images = int(0.2 * len(images))\n",
    "    num_validaton_images = int(0.2 * len(images))\n",
    "\n",
    "    # Copy images to the testing dataset\n",
    "    for image in images[:num_test_images]:\n",
    "        src_image_path = os.path.join(disease_path, image)\n",
    "        dst_image_path = os.path.join(test_folders, image)\n",
    "        shutil.copy(src_image_path, dst_image_path)\n",
    "\n",
    "    # copy images to the validation dataset\n",
    "    for image in images[num_test_images:num_validaton_images+num_test_images]:\n",
    "        src_image_path = os.path.join(disease_path, image)\n",
    "        dst_image_path = os.path.join(validation_folders, image)\n",
    "        shutil.copy(src_image_path, dst_image_path)\n",
    "\n",
    "    # copy the remaining images to the training dataset\n",
    "    for image in images[num_test_images:]:\n",
    "        src_image_path = os.path.join(disease_path, image)\n",
    "        dst_image_path = os.path.join(train_folders, image)\n",
    "        shutil.copy(src_image_path, dst_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow-addons\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 1060, 1479\n",
    "batch_size = 8\n",
    "\n",
    "train_dir = '/content/drive/MyDrive/datasets/EKG/datasetOriginal/train'\n",
    "test_dir = '/content/drive/MyDrive/datasets/EKG/datasetOriginal/test'\n",
    "validation_dir = '/content/drive/MyDrive/datasets/EKG/datasetOriginal/validation'\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_vgg16 = VGG16(weights='imagenet', include_top=False, input_shape=(img_height, img_width, 3))\n",
    "base_vgg16.trainable = False\n",
    "\n",
    "for layer in base_vgg16.layers[-4:]:\n",
    "  layer.trainable = True\n",
    "\n",
    "x = base_vgg16.output\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2,2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu')(x)\n",
    "x = layers.MaxPooling2D((2,2))(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "# x = layers.Flatten()(x)  # Flatten layer to convert 2D output to 1D\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.02))(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Dense(8, activation='relu')(x)\n",
    "predictions = layers.Dense(2, activation='softmax')(x)\n",
    "\n",
    "vgg16_model = tf.keras.models.Model(inputs = base_vgg16.input, outputs= predictions)\n",
    "vgg16_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tfa.optimizers.AdaBelief(learning_rate=1e-3), metrics=[\"accuracy\", tf.keras.metrics.Recall()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    \"/content/drive/MyDrive/datasets/EKG/dataset300/model_checkpointtfaSigmoid.h5\",  # Path to save the checkpoint\n",
    "    save_best_only=True,    # Save only the best model\n",
    "    monitor=\"val_loss\",     # Metric to monitor (e.g., validation loss)\n",
    "    mode=\"min\",             # Minimize the monitored metric\n",
    "    verbose=1                # Verbosity level\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = vgg16_model.fit(train_generator, epochs=100, validation_data=validation_generator,callbacks=[checkpoint_callback])\n",
    "vgg16_model.evaluate(test_generator)\n",
    "# loss: 0.1567 - accuracy: 0.9531 - recall_2: 0.9531"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
